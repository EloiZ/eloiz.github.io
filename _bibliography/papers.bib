---
---
@article{feng2026rap,
  bibtex_show={true},
  preview={rap.gif},
  code={https://github.com/vita-epfl/RAP},
  arxiv={2510.04333},
  abstract={Lightweight 3D rasterized views can augment imitation learning data for end-to-end driving.},
  title     = {{RAP}: 3D Rasterization Augmented End-to-End Planning},
  author    = {Lan Feng and
               Yang Gao and
               Éloi Zablocki and
               Quanyi Li and
               Wuyang Li and
               Sichao Liu and
               Matthieu Cord and
               Alexandre Alahi},
  journal = {preprint},
  year    = {2026}
}

@inproceedings{bartoccioni2025vavim-vavam,
  bibtex_show={true},
  preview={vavim_vavam.png},
  code={https://github.com/valeoai/VideoActionModel},
  arxiv={2502.15672},
  abstract={Learning to drive from YouTube. VaViM, a 1.2B parameter video generative model trained on 1,800+ hours of raw YouTube driving videos, enables VaVAM, a video-action model that achieves state-of-the-art results on the NeuroNCAP driving benchmark.},
  title={{VaViM and VaVAM}: Autonomous Driving through Video Generative Modeling},
  author={Florent Bartoccioni and Elias Ramzi and Victor Besnier and Shashanka Venkataramanan and Tuan-Hung Vu and Yihong Xu and Loick Chambon and Spyros Gidaris and Serkan Odabas and David Hurych and Renaud Marlet and Alexandre Boulch and Mickael Chen and Éloi Zablocki and Andrei Bursuc and Eduardo Valle and Matthieu Cord},
  booktitle={CoRL Workshop on Learning to Simulate Robot Worlds},
  year={2025}
}


@inproceedings{chambon2025gaussrender,
  bibtex_show={true},
  preview={gaussrender.gif},
  code={https://github.com/valeoai/gaussrender},
  arxiv={2502.05040},
  abstract={A plug-and-play 3D-to-2D projective consistency loss using Gaussian splatting enhances 3D semantic occupancy learning from multiple cameras.},
  title     = {{GaussRender}: Learning 3D Occupancy with Gaussian Rendering},
  author    = {Loick Chambon and
               Éloi Zablocki and
               Alexandre Boulch and
               Mickaël Chen and
               Matthieu Cord},
  booktitle = {ICCV},
  year    = {2025}
}

@article{zablocki2025gift,
    bibtex_show={true},
    preview={2025_gift.PNG},
    arxiv={2411.15605},
    code={https://github.com/valeoai/GIFT},
    abstract={A framework for generating global, interpretable textual explanations of vision classifiers, combining counterfactual visual explanations with VLMs and LLMs.},
    title = {{GIFT}: A Framework for Global Interpretable Faithful Textual Explanations of Vision Classifiers},
    author = {Éloi Zablocki* and
              Valentin Gerard* and
              Amaia Cardiel and
              Eric Gaussier and
              Matthieu Cord and
              Eduardo Valle
            },
    journal = {under review},
    year = {2025}
}

@inproceedings{xu2025ppt,
  bibtex_show={true},
  preview={ppt.PNG},
  arxiv={2412.06491},
  code={https://github.com/valeoai/PPT},
  abstract={Pre-training with pseudo-labeled trajectories, obtained with offline 3D-trackers, boosts trajectory prediction models: improved performance, efficiency, and generalization.},
  title={{PPT}: Pre-Training with Pseudo-Labeled Trajectories for Motion Forecasting},
  author    = {Yihong Xu and
               Yuan Yin and
               Éloi Zablocki and
               Tuan-Hung Vu and
               Alexandre Boulch and
               Matthieu Cord},
  booktitle = {CoRL Workshop on Making Sense of Data in Robotics},
  year = {2025}
}


@inproceedings{xu2025awta,
  bibtex_show={true},
  preview={awta.PNG},
  code={https://github.com/valeoai/MF_aWTA},
  video={https://www.youtube.com/watch?v=EOsIayPi7Lw},
  arxiv={2409.11172},
  abstract={Using an annealing loss enhances training stability and performance of state-of-the-art trajectory prediction models.},
  title={Annealed Winner-Takes-All for Motion Forecasting},
  author    = {Yihong Xu and
               Victor Letzelter and
               Mickaël Chen and
               Éloi Zablocki and
               Matthieu Cord},
  booktitle = {ICRA},
  year = {2025}
}


@inproceedings{cardiel2025llm-wrapper,
  bibtex_show={true},
  preview={llm_wrapper.png},
  code={https://github.com/valeoai/llm_wrapper},
  arxiv={2409.11919},
  abstract={LLMs can learn to adapt black-box VLMs for new tasks and domains, by wrapping and reasoning on the vision models' outputs.},
  title={{LLM-wrapper}: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension},
  author = {Amaia Cardiel and
           Éloi Zablocki and
           Oriane Simeoni and
           Elias Ramzi and
           Matthieu Cord},
  booktitle = {ICLR},
  year = {2025}
}

@inproceedings{yin2024regents,
  bibtex_show={true},
  preview={regents_page.png},
  code={https://github.com/valeoai/regents},
  arxiv={2409.07830},
  abstract={ReGentS generates safety-critical driving scenarios with adversarial optimization of real-world trajectories.},
  title={{ReGentS}: Real-World Safety-Critical Driving Scenario Generation Made Stable},
  author = {Yuan Yin and
           Pegah Khayatan and
           Éloi Zablocki and
           Alexandre Boulch and
           Matthieu Cord
           },
  booktitle = {ECCV Workshop W-CODA},
  year = {2024}
}

@inproceedings{xu2024valeo4cast,
  bibtex_show={true},
  preview={valeo4cast.png},
  code={https://github.com/valeoai/valeo4cast},
  arxiv={2406.08113},
  video={https://www.youtube.com/watch?v=2BvoPbW4vpc&t=1107s},
  abstract={Using separate training and fine-tuning of detection, tracking, and forecasting modules, achieves first place in the Argoverse 2 Challenge, outperforming last year's winner by +17.1 points.},
  title={{Valeo4Cast}: A Modular Approach to End-to-End Forecasting},
  author={Xu*, Yihong and Zablocki*, Éloi and Boulch*, Alexandre and Puy, Gilles and Chen, Mickael and Bartoccioni, Florent and Samet, Nermin and Sim{\'e}oni, Oriane and Gidaris, Spyros and Vu, Tuan-Hung and Bursuc, Andrei and Valle, Eduardo and Marlet, Renaud and Cord, Matthieu},
  booktitle={ECCV Workshop ROAD++},
  year={2024},
  award_name={Challenge winning solution},
  award={Winning solution for the Argoverse-2 end-to-end forecasting challenge, held at the CVPR WAD workshop}
}

@inproceedings{feng2024unitraj,
  bibtex_show={true},
  preview={unitraj.gif},
  code={https://github.com/vita-epfl/unitraj},
  video={https://www.youtube.com/watch?v=2IzuUtiNA_4},
  arxiv={2403.15098},
  abstract={Unifying major datasets for vehicle trajectory prediction enables the study of scale and diversity impacts on performance and model generalization.},
  title     = {{UniTraj}: A Unified Framework for Scalable Vehicle Trajectory Prediction},
  author    = {Lan Feng and
               Mohammadhossein Bahari and
               Kaouther Messaoud Ben Amor and
               Éloi Zablocki and
               Matthieu Cord and
               Alexandre Alahi},
  booktitle = {ECCV},
  year    = {2024}
}


@inproceedings{chambon2024pointbev,
  bibtex_show={true},
  preview={pointbev.png},
  code={https://github.com/valeoai/pointbev},
  arxiv={2312.00703},
  abstract={A sparse approach to bird's-eye view perception enhances performance and computational efficiency by avoiding the uniform allocation of resources across all cells, making it flexible to the task, situation and compute budget at inference time.},
  title     = {{PointBeV}: A Sparse Approach to BeV Predictions},
  author    = {Loick Chambon and
               Éloi Zablocki and
               Mickaël Chen and
               Florent Bartoccioni and
               Matthieu Cord and
               Patrick Pérez},
  booktitle = {CVPR},
  year    = {2024}
}


@inproceedings{simeoni2024unsupervised,
  bibtex_show={true},
  preview={unsup_obj_loc_survey.png},
  code={https://github.com/valeoai/Awesome-Unsupervised-Object-Localization},
  arxiv={2310.12904},
  abstract={A survey on unsupervised object localization methods leveraging self-supervised pre-trained features, e.g., DINO.},
author       = {Oriane Siméoni and
             Éloi Zablocki and
                  Spyros Gidaris and
                  Gilles Puy and
                  Patrick Pérez},
  title        = {Unsupervised Object Localization in the Era of Self-Supervised ViTs:
                  {A} Survey},
  booktitle      = {IJCV},
  year         = {2024}
}

@inproceedings{xu2024towards,
  bibtex_show={true},
  preview={e2e_forecasting.png},
  code={https://github.com/valeoai/MFEval},
  arxiv={2306.09281},
  abstract={This work presents a unified evaluation pipeline for motion forecasting with real-world perception inputs, revealing a performance gap between curated and perception-based data.},
  title     = {Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive?},
  author    = {Yihong Xu and
               Loick Chambon and
               Éloi Zablocki and
               Mickaël Chen and
               Alexandre Alahi and
               Matthieu Cord and
               Patrick Pérez},
  booktitle = {ICRA},
  year    = {2024}
}

@inproceedings{zemni2023octet,
  bibtex_show={true},
  preview={octet.png},
  video={https://youtu.be/Xfq0uRcw9jQ},
  code={https://github.com/valeoai/octet},
  arxiv={2211.12380},
  abstract={Using a spatial- and object-aware generative model enables the generation of counterfactual explanations for deep vision models dealing with complex scenes, including many objects.},
  author    = {Mehdi Zemni and
                Mickaël Chen and
                Éloi Zablocki and
                Hédi Ben-Younes and
               Patrick Pérez and
               Matthieu Cord},
  title     = {{OCTET}: Object-aware Counterfactual Explanations},
  booktitle = {CVPR},
  year      = {2023}
}

@inproceedings{simeoni2023found,
    bibtex_show={true},
  preview={found.png},
  video={https://www.youtube.com/watch?v=jfYQfFcrJBE},
  code={https://github.com/valeoai/found},
  arxiv={2212.07834},
  abstract={FOUND trains a single conv1x1 on DINO features, for unsupervised object segmentation. It runs at 80 FPS on a V100 after a 2h self-training on a single GPU.},
  author    = {Oriane Siméoni and
               Chloé Sekkat and
               Gilles Puy and
               Antonín Vobecký and
               Éloi Zablocki and
               Patrick Pérez},
  title     = {Unsupervised Object Localization: Observing the Background to Discover Objects},
  booktitle = {CVPR},
  year      = {2023}
}

@inproceedings{bartoccioni2023lidartouch,
  bibtex_show={true},
  preview={lidartouch.png},
  code={https://github.com/valeoai/lidartouch},
  arxiv={2109.03569},
  abstract={Adding a low-cost LiDAR to a monocular camera setup yields improved metric depth maps in a self-supervised manner.},
  author    = {Florent Bartoccioni and
               Éloi Zablocki and
               Patrick Pérez and
               Matthieu Cord and
               Karteek Alahari},
  title     = {{LiDARTouch}: Monocular metric depth estimation with a few-beam LiDAR},
  booktitle = {CVIU},
  year      = {2023}
}


@inproceedings{bartoccioni2022lara,
  bibtex_show={true},
  preview={lara.png},
  code={https://github.com/valeoai/lara},
  arxiv={2206.13294},
  abstract={The Perceiver architecture, combined with careful ray encoding, excels in multi-camera fusion and transforming perceptive views into bird's-eye-view semantic segmentation.},
  author    = {Florent Bartoccioni and
               Éloi Zablocki and
               Andrei Bursuc and
               Patrick Pérez and
               Matthieu Cord and
               Karteek Alahari},
  title     = {{LaRa}: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation},
  booktitle = {CoRL},
  year      = {2022}
}

@inproceedings{jacob2022steex,
  bibtex_show={true},
  preview={steex.png},
  video={https://youtu.be/79SMlEtscuY},
  code={https://github.com/valeoai/steex},
  arxiv={2111.09094},
  abstract={Using a well-structured image generative model unlocks the generation of counterfactual explanations for deep vision models dealing with high-quality image and complex scenes.},
  author    = {Paul Jacob and
                Éloi Zablocki and
                Hédi Ben-Younes and
                Mickaël Chen and
               Patrick Pérez and
               Matthieu Cord},
  title     = {{STEEX}: Steering Counterfactual Explanations with Semantics},
  booktitle = {ECCV},
  year      = {2022}
}

@inproceedings{benyounes2022cab,
  bibtex_show={true},
  preview={cab.png},
  code={https://github.com/valeoai/cab},
  arxiv={2109.08048},
  abstract={As trajectory prediction models merely extrapolate past motion, CAB enhances the use of HD-map information, addressing long-tail corner cases.},
  author    = {Hedi Ben-Younes* and
               Éloi Zablocki* and
               Mickaël Chen and
               Matthieu Cord and
               Patrick Pérez},
  title     = {Raising context awareness in motion forecasting},
  booktitle = {CVPR Workshop on Autonomous Driving (WAD)},
  year      = {2022}
}

@inproceedings{zablocki2022xai_driving_survey,
  bibtex_show={true},
  preview={xai_survey.png},
  arxiv={2101.05307},
  abstract={A survey on explainability methods for vision-based autonomous-driving models.},
  author    = {Éloi Zablocki* and
                Hédi Ben-Younes* and
               Patrick Pérez and
               Matthieu Cord},
  title     = {Explainability of deep vision-based autonomous driving systems: Review and challenges},
  booktitle = {IJCV},
  year      = {2022}
}

@inproceedings{benyounes2022beef,
  bibtex_show={true},
  preview={beef.png},
  code={https://github.com/valeoai/beef},
  arxiv={2012.04983},
  abstract={BEEF is a self-driving model that both drives and explains its decisions with natural language.},
  author    = {Hedi Ben-Younes* and
               Éloi Zablocki* and
               Matthieu Cord and
               Patrick Pérez},
  title     = {Driving Behavior Explanation with Multi-level Fusion},
  booktitle = {Pattern Recognition journal (PR)},
  year      = {2022}
}

@article{bordes2020transductive,
  bibtex_show={true},
  preview={transductive_zsl.PNG},
  arxiv={2011.06850},
  abstract={Using a cycle-consistency loss reduces the domain shift between visual and textual representations, enhancing performance in zero-shot object recognition.},
  author    = {Patrick Bordes and
               Éloi Zablocki and
               Benjamin Piwowarski and
               Patrick Gallinari},
  title     = {Transductive Zero-Shot Learning using Cross-modal {CycleGAN}},
  journal = {arxiv},
  year      = {2020}
}

@inproceedings{zablocki2019context,
  bibtex_show={true},
  preview={czsl.PNG},
  arxiv={1904.12638},
  abstract={Using visual context boosts zero-shot object recognition.},
  author    = {Éloi Zablocki* and
               Patrick Bordes* and
               Benjamin Piwowarski and
               Laure Soulier and
               Patrick Gallinari},
  title     = {Context-Aware Zero-Shot Learning for Object Recognition},
  booktitle = {ICML},
  year      = {2019},
  award     = {Extended Oral},
  award_name= {Extended Oral}
}

@inproceedings{bordes2019incorporating,
  bibtex_show={true},
  preview={grounded_sentence_embedding.PNG},
  arxiv={2002.02734},
  abstract={A careful transfer of visual features to sentence representations enriches the semantics of general-purpose textual representations.},
  author    = {Patrick Bordes* and
               Éloi Zablocki* and
               Laure Soulier and
               Benjamin Piwowarski and
               Patrick Gallinari},
  title     = {Incorporating Visual Semantics into Sentence Representations within a Grounded Space},
  booktitle = {EMNLP},
  year      = {2019},
  award     = {Oral},
  award_name= {Oral}
}

@inproceedings{zablocki2018learning,
  bibtex_show={true},
  preview={grounded_word_embedding.PNG},
  arxiv={1711.03483},
  abstract={Visual context can be used, along with textual context, to learn improved word representations with the skip-gram algorithm.}, 
  author    = {Éloi Zablocki and
               Benjamin Piwowarski and
               Laure Soulier and
               Patrick Gallinari},
  title     = {Learning Multi-Modal Word Representation Grounded in Visual Context},
  booktitle = {AAAI},
  year      = {2018}
}

@inproceedings{zablocki2017sprl,
  bibtex_show={true},
  preview={clef.PNG},
  abstract={A linear SVM on pooled word representations to classify spatial relations from text and images.},
  pdf={https://ceur-ws.org/Vol-1866/paper_76.pdf},
  author    = {Éloi Zablocki and
               Patrick Bordes and
               Laure Soulier and
               Benjamin Piwowarski and
               Patrick Gallinari},
  title     = {{LIP6@CLEF2017}: Multi-Modal Spatial Role Labeling using Word Embeddings},
  booktitle = {CLEF},
  year      = {2017}
}

