<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Éloi Zablocki </title> <meta name="author" content="Éloi Zablocki"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ez_favicon/favicon.ico?635783a3c57963f088a739a87b3ec162"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://eloiz.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Éloi</span> Zablocki </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gaussrender.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/gaussrender.PNG" class="preview z-depth-1" width="100%" height="auto" alt="gaussrender.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chambon2025gaussrender" class="col-sm-8"> <div class="title">GaussRender: Learning 3D Occupancy with Gaussian Rendering</div> <div class="author"> <a href="https://loickch.github.io/" rel="external nofollow noopener" target="_blank">Loick Chambon</a>, <em>Éloi Zablocki</em>, <a href="https://boulch.eu/" rel="external nofollow noopener" target="_blank">Alexandre Boulch</a>, <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" rel="external nofollow noopener" target="_blank">Mickaël Chen</a>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>under review</em>, 2025 </div> <div class="periodical"> </div> <div class="abstract"> <em> A plug-and-play 3D-to-2D reprojection loss using Gaussian splatting enhances 3D semantic occupancy learning from multiple cameras. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2502.05040" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/gaussrender" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chambon2025gaussrender</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{GaussRender}: Learning 3D Occupancy with Gaussian Rendering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chambon, Loick and Zablocki, Éloi and Boulch, Alexandre and Chen, Mickaël and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{under review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025_gift.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/2025_gift.PNG" class="preview z-depth-1" width="100%" height="auto" alt="2025_gift.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zablocki2025gift" class="col-sm-8"> <div class="title">GIFT: A Framework for Global Interpretable Faithful Textual Explanations of Vision Classifiers</div> <div class="author"> <em>Éloi Zablocki<sup>*</sup></em>, <a href="https://www.linkedin.com/in/valentingerard100/" rel="external nofollow noopener" target="_blank">Valentin Gerard<sup>*</sup></a>, <a href="https://www.linkedin.com/in/amaia-cardiel-a63489aa/" rel="external nofollow noopener" target="_blank">Amaia Cardiel</a>, <a href="https://ama.liglab.fr/~gaussier/" rel="external nofollow noopener" target="_blank">Eric Gaussier</a>, <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a>, and <a href="https://eduardovalle.com/" rel="external nofollow noopener" target="_blank">Eduardo Valle</a> </div> <div class="periodical"> <em>under review</em>, 2025 </div> <div class="periodical"> </div> <div class="abstract"> <em> A framework for generating global, interpretable textual explanations of vision classifiers, combining counterfactual visual explanations with VLMs and LLMs. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2411.15605" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/GIFT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zablocki2025gift</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{GIFT}: A Framework for Global Interpretable Faithful Textual Explanations of Vision Classifiers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zablocki, Éloi and Gerard, Valentin and Cardiel, Amaia and Gaussier, Eric and Cord, Matthieu and Valle, Eduardo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{under review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ppt.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/ppt.PNG" class="preview z-depth-1" width="100%" height="auto" alt="ppt.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2025ppt" class="col-sm-8"> <div class="title">PPT: Pre-Training with Pseudo-Labeled Trajectories for Motion Forecasting</div> <div class="author"> <a href="https://scholar.google.fr/citations?user=vMLRRVkAAAAJ" rel="external nofollow noopener" target="_blank">Yihong Xu</a>, <a href="https://yuan-yin.github.io/" rel="external nofollow noopener" target="_blank">Yuan Yin</a>, <a href="https://tuanhungvu.github.io/" rel="external nofollow noopener" target="_blank">Tuan-Hung Vu</a>, <a href="https://boulch.eu/" rel="external nofollow noopener" target="_blank">Alexandre Boulch</a>, <em>Éloi Zablocki</em>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>under review</em>, 2025 </div> <div class="periodical"> </div> <div class="abstract"> <em> Pre-training with pseudo-labeled trajectories, obtained with offline 3D-trackers, boosts trajectory prediction models: improved performance, efficiency, and generalization. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2412.06491" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/PPT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xu2025ppt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{PPT}: Pre-Training with Pseudo-Labeled Trajectories for Motion Forecasting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Yihong and Yin, Yuan and Vu, Tuan-Hung and Boulch, Alexandre and Zablocki, Éloi and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{under review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/awta-480.webp 480w,/assets/img/publication_preview/awta-800.webp 800w,/assets/img/publication_preview/awta-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/awta.png" class="preview z-depth-1" width="100%" height="auto" alt="awta.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2025awta" class="col-sm-8"> <div class="title">Annealed Winner-Takes-All for Motion Forecasting</div> <div class="author"> <a href="https://scholar.google.fr/citations?user=vMLRRVkAAAAJ" rel="external nofollow noopener" target="_blank">Yihong Xu</a>, <a href="https://victorletzelter.github.io/" rel="external nofollow noopener" target="_blank">Victor Letzelter</a>, <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" rel="external nofollow noopener" target="_blank">Mickaël Chen</a>, <em>Éloi Zablocki</em>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>In ICRA</em>, 2025 </div> <div class="periodical"> </div> <div class="abstract"> <em> Using an annealing loss enhances training stability and performance of state-of-the-art trajectory prediction models. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2409.11172" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=EOsIayPi7Lw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/valeoai/MF_aWTA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2025awta</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Annealed Winner-Takes-All for Motion Forecasting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Yihong and Letzelter, Victor and Chen, Mickaël and Zablocki, Éloi and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/llm_wrapper-480.webp 480w,/assets/img/publication_preview/llm_wrapper-800.webp 800w,/assets/img/publication_preview/llm_wrapper-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/llm_wrapper.png" class="preview z-depth-1" width="100%" height="auto" alt="llm_wrapper.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cardiel2025llm-wrapper" class="col-sm-8"> <div class="title">LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension</div> <div class="author"> <a href="https://www.linkedin.com/in/amaia-cardiel-a63489aa/" rel="external nofollow noopener" target="_blank">Amaia Cardiel</a>, <em>Éloi Zablocki</em>, <a href="https://osimeoni.github.io/" rel="external nofollow noopener" target="_blank">Oriane Simeoni</a>, <a href="https://elias-ramzi.github.io/" rel="external nofollow noopener" target="_blank">Elias Ramzi</a>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>In ICLR</em>, 2025 </div> <div class="periodical"> </div> <div class="abstract"> <em> LLMs can learn to adapt black-box VLMs for new tasks and domains, by wrapping and reasoning on the vision models’ outputs. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2409.11919" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/llm_wrapper" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cardiel2025llm-wrapper</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{LLM-wrapper}: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cardiel, Amaia and Zablocki, Éloi and Simeoni, Oriane and Ramzi, Elias and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/regents_page-480.webp 480w,/assets/img/publication_preview/regents_page-800.webp 800w,/assets/img/publication_preview/regents_page-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/regents_page.png" class="preview z-depth-1" width="100%" height="auto" alt="regents_page.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yin2024regents" class="col-sm-8"> <div class="title">ReGentS: Real-World Safety-Critical Driving Scenario Generation Made Stable</div> <div class="author"> <a href="https://yuan-yin.github.io/" rel="external nofollow noopener" target="_blank">Yuan Yin</a>, <a href="https://pegah-kh.github.io/" rel="external nofollow noopener" target="_blank">Pegah Khayatan</a>, <em>Éloi Zablocki</em>, <a href="https://boulch.eu/" rel="external nofollow noopener" target="_blank">Alexandre Boulch</a>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>In ECCV Workshop W-CODA</em>, 2024 </div> <div class="periodical"> </div> <div class="abstract"> <em> ReGentS generates safety-critical driving scenarios with adversarial optimization of real-world trajectories. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2409.07830" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/regents" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yin2024regents</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{ReGentS}: Real-World Safety-Critical Driving Scenario Generation Made Stable}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yin, Yuan and Khayatan, Pegah and Zablocki, Éloi and Boulch, Alexandre and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV Workshop W-CODA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/valeo4cast-480.webp 480w,/assets/img/publication_preview/valeo4cast-800.webp 800w,/assets/img/publication_preview/valeo4cast-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/valeo4cast.png" class="preview z-depth-1" width="100%" height="auto" alt="valeo4cast.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2024valeo4cast" class="col-sm-8"> <div class="title">Valeo4Cast: A Modular Approach to End-to-End Forecasting</div> <div class="author"> <a href="https://scholar.google.fr/citations?user=vMLRRVkAAAAJ" rel="external nofollow noopener" target="_blank">Yihong Xu<sup>*</sup></a>, <em>Éloi Zablocki<sup>*</sup></em>, <a href="https://boulch.eu/" rel="external nofollow noopener" target="_blank">Alexandre Boulch<sup>*</sup></a>, <a href="https://sites.google.com/site/puygilles/" rel="external nofollow noopener" target="_blank">Gilles Puy</a>, <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" rel="external nofollow noopener" target="_blank">Mickael Chen</a>, <a href="https://f-barto.github.io/" rel="external nofollow noopener" target="_blank">Florent Bartoccioni</a>, <a href="https://nerminsamet.github.io/" rel="external nofollow noopener" target="_blank">Nermin Samet</a>, <a href="https://osimeoni.github.io/" rel="external nofollow noopener" target="_blank">Oriane Siméoni</a>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ" rel="external nofollow noopener" target="_blank">Spyros Gidaris</a>, <a href="https://tuanhungvu.github.io/" rel="external nofollow noopener" target="_blank">Tuan-Hung Vu</a>, <a href="https://abursuc.github.io/" rel="external nofollow noopener" target="_blank">Andrei Bursuc</a>, <a href="https://eduardovalle.com/" rel="external nofollow noopener" target="_blank">Eduardo Valle</a>, <a href="https://imagine.enpc.fr/~marletr/" rel="external nofollow noopener" target="_blank">Renaud Marlet</a>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>In ECCV Workshop ROAD++</em>, 2024 </div> <div class="periodical"> </div> <div class="abstract"> <em> Using separate training and fine-tuning of detection, tracking, and forecasting modules, achieves first place in the Argoverse 2 Challenge, outperforming last year’s winner by +17.1 points. </em> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Challenge winning solution</a> <a href="http://arxiv.org/abs/2406.08113" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=2BvoPbW4vpc&amp;t=1107s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/valeoai/valeo4cast" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Winning solution for the Argoverse-2 end-to-end forecasting challenge, held at the CVPR WAD workshop</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2024valeo4cast</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Valeo4Cast}: A Modular Approach to End-to-End Forecasting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Yihong and Zablocki, Éloi and Boulch, Alexandre and Puy, Gilles and Chen, Mickael and Bartoccioni, Florent and Samet, Nermin and Sim{\'e}oni, Oriane and Gidaris, Spyros and Vu, Tuan-Hung and Bursuc, Andrei and Valle, Eduardo and Marlet, Renaud and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV Workshop ROAD++}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unitraj-480.webp 480w,/assets/img/publication_preview/unitraj-800.webp 800w,/assets/img/publication_preview/unitraj-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/unitraj.gif" class="preview z-depth-1" width="100%" height="auto" alt="unitraj.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="feng2024unitraj" class="col-sm-8"> <div class="title">UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction</div> <div class="author"> <a href="https://alan-lanfeng.github.io/" rel="external nofollow noopener" target="_blank">Lan Feng</a>, <a href="https://mohammadhossein-bahari.github.io/" rel="external nofollow noopener" target="_blank">Mohammadhossein Bahari</a>, <a href="https://scholar.google.com/citations?user=X0teZIAAAAAJ" rel="external nofollow noopener" target="_blank">Kaouther Messaoud Ben Amor</a>, <em>Éloi Zablocki</em>, <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a>, and <a href="https://people.epfl.ch/alexandre.alahi" rel="external nofollow noopener" target="_blank">Alexandre Alahi</a> </div> <div class="periodical"> <em>In ECCV</em>, 2024 </div> <div class="periodical"> </div> <div class="abstract"> <em> Unifying major datasets for vehicle trajectory prediction enables the study of scale and diversity impacts on performance and model generalization. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2403.15098" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=2IzuUtiNA_4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/vita-epfl/unitraj" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">feng2024unitraj</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{UniTraj}: A Unified Framework for Scalable Vehicle Trajectory Prediction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Feng, Lan and Bahari, Mohammadhossein and Amor, Kaouther Messaoud Ben and Zablocki, Éloi and Cord, Matthieu and Alahi, Alexandre}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pointbev-480.webp 480w,/assets/img/publication_preview/pointbev-800.webp 800w,/assets/img/publication_preview/pointbev-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/pointbev.png" class="preview z-depth-1" width="100%" height="auto" alt="pointbev.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chambon2024pointbev" class="col-sm-8"> <div class="title">PointBeV: A Sparse Approach to BeV Predictions</div> <div class="author"> <a href="https://loickch.github.io/" rel="external nofollow noopener" target="_blank">Loick Chambon</a>, <em>Éloi Zablocki</em>, <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" rel="external nofollow noopener" target="_blank">Mickaël Chen</a>, <a href="https://f-barto.github.io/" rel="external nofollow noopener" target="_blank">Florent Bartoccioni</a>, <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a>, and <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a> </div> <div class="periodical"> <em>In CVPR</em>, 2024 </div> <div class="periodical"> </div> <div class="abstract"> <em> A sparse approach to bird’s-eye view perception enhances performance and computational efficiency by avoiding the uniform allocation of resources across all cells, making it flexible to the task, situation and compute budget at inference time. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2312.00703" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/pointbev" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chambon2024pointbev</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{PointBeV}: A Sparse Approach to BeV Predictions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chambon, Loick and Zablocki, Éloi and Chen, Mickaël and Bartoccioni, Florent and Cord, Matthieu and Pérez, Patrick}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unsup_obj_loc_survey-480.webp 480w,/assets/img/publication_preview/unsup_obj_loc_survey-800.webp 800w,/assets/img/publication_preview/unsup_obj_loc_survey-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/unsup_obj_loc_survey.png" class="preview z-depth-1" width="100%" height="auto" alt="unsup_obj_loc_survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="simeoni2024unsupervised" class="col-sm-8"> <div class="title">Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey</div> <div class="author"> <a href="https://osimeoni.github.io/" rel="external nofollow noopener" target="_blank">Oriane Siméoni</a>, <em>Éloi Zablocki</em>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ" rel="external nofollow noopener" target="_blank">Spyros Gidaris</a>, <a href="https://sites.google.com/site/puygilles/" rel="external nofollow noopener" target="_blank">Gilles Puy</a>, and <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a> </div> <div class="periodical"> <em>In IJCV</em>, 2024 </div> <div class="periodical"> </div> <div class="abstract"> <em> A survey on unsupervised object localization methods leveraging self-supervised pre-trained features, e.g., DINO. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2310.12904" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/Awesome-Unsupervised-Object-Localization" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">simeoni2024unsupervised</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Siméoni, Oriane and Zablocki, Éloi and Gidaris, Spyros and Puy, Gilles and Pérez, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unsupervised Object Localization in the Era of Self-Supervised ViTs:
                    {A} Survey}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IJCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/e2e_forecasting-480.webp 480w,/assets/img/publication_preview/e2e_forecasting-800.webp 800w,/assets/img/publication_preview/e2e_forecasting-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/e2e_forecasting.png" class="preview z-depth-1" width="100%" height="auto" alt="e2e_forecasting.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2024towards" class="col-sm-8"> <div class="title">Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive?</div> <div class="author"> <a href="https://scholar.google.fr/citations?user=vMLRRVkAAAAJ" rel="external nofollow noopener" target="_blank">Yihong Xu</a>, <a href="https://loickch.github.io/" rel="external nofollow noopener" target="_blank">Loick Chambon</a>, <em>Éloi Zablocki</em>, <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" rel="external nofollow noopener" target="_blank">Mickaël Chen</a>, <a href="https://people.epfl.ch/alexandre.alahi" rel="external nofollow noopener" target="_blank">Alexandre Alahi</a>, <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a>, and <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a> </div> <div class="periodical"> <em>In ICRA</em>, 2024 </div> <div class="periodical"> </div> <div class="abstract"> <em> This work presents a unified evaluation pipeline for motion forecasting with real-world perception inputs, revealing a performance gap between curated and perception-based data. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2306.09281" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/MFEval" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2024towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Motion Forecasting with Real-World Perception Inputs: Are End-to-End Approaches Competitive?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Yihong and Chambon, Loick and Zablocki, Éloi and Chen, Mickaël and Alahi, Alexandre and Cord, Matthieu and Pérez, Patrick}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICRA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/octet-480.webp 480w,/assets/img/publication_preview/octet-800.webp 800w,/assets/img/publication_preview/octet-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/octet.png" class="preview z-depth-1" width="100%" height="auto" alt="octet.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zemni2023octet" class="col-sm-8"> <div class="title">OCTET: Object-aware Counterfactual Explanations</div> <div class="author"> <a href="https://scholar.google.fr/citations?user=2IKUaWAAAAAJ" rel="external nofollow noopener" target="_blank">Mehdi Zemni</a>, <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" rel="external nofollow noopener" target="_blank">Mickaël Chen</a>, <em>Éloi Zablocki</em>, <a href="https://scholar.google.fr/citations?user=IFLcfvUAAAAJ" rel="external nofollow noopener" target="_blank">Hédi Ben-Younes</a>, <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>In CVPR</em>, 2023 </div> <div class="periodical"> </div> <div class="abstract"> <em> Using a spatial- and object-aware generative model enables the generation of counterfactual explanations for deep vision models dealing with complex scenes, including many objects. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2211.12380" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/Xfq0uRcw9jQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/valeoai/octet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zemni2023octet</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zemni, Mehdi and Chen, Mickaël and Zablocki, Éloi and Ben-Younes, Hédi and Pérez, Patrick and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{OCTET}: Object-aware Counterfactual Explanations}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/found-480.webp 480w,/assets/img/publication_preview/found-800.webp 800w,/assets/img/publication_preview/found-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/found.png" class="preview z-depth-1" width="100%" height="auto" alt="found.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="simeoni2023found" class="col-sm-8"> <div class="title">Unsupervised Object Localization: Observing the Background to Discover Objects</div> <div class="author"> <a href="https://osimeoni.github.io/" rel="external nofollow noopener" target="_blank">Oriane Siméoni</a>, <a href="https://github.com/chloeskt" rel="external nofollow noopener" target="_blank">Chloé Sekkat</a>, <a href="https://sites.google.com/site/puygilles/" rel="external nofollow noopener" target="_blank">Gilles Puy</a>, <a href="https://vobecant.github.io/" rel="external nofollow noopener" target="_blank">Antonín Vobecký</a>, <em>Éloi Zablocki</em>, and <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a> </div> <div class="periodical"> <em>In CVPR</em>, 2023 </div> <div class="periodical"> </div> <div class="abstract"> <em> FOUND trains a single conv1x1 on DINO features, for unsupervised object segmentation. It runs at 80 FPS on a V100 after a 2h self-training on a single GPU. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2212.07834" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=jfYQfFcrJBE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/valeoai/found" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">simeoni2023found</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Siméoni, Oriane and Sekkat, Chloé and Puy, Gilles and Vobecký, Antonín and Zablocki, Éloi and Pérez, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unsupervised Object Localization: Observing the Background to Discover Objects}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lidartouch-480.webp 480w,/assets/img/publication_preview/lidartouch-800.webp 800w,/assets/img/publication_preview/lidartouch-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/lidartouch.png" class="preview z-depth-1" width="100%" height="auto" alt="lidartouch.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bartoccioni2023lidartouch" class="col-sm-8"> <div class="title">LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR</div> <div class="author"> <a href="https://f-barto.github.io/" rel="external nofollow noopener" target="_blank">Florent Bartoccioni</a>, <em>Éloi Zablocki</em>, <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a>, and <a href="https://lear.inrialpes.fr/people/alahari/" rel="external nofollow noopener" target="_blank">Karteek Alahari</a> </div> <div class="periodical"> <em>In CVIU</em>, 2023 </div> <div class="periodical"> </div> <div class="abstract"> <em> Adding a low-cost LiDAR to a monocular camera setup yields improved metric depth maps in a self-supervised manner. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2109.03569" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/lidartouch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bartoccioni2023lidartouch</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bartoccioni, Florent and Zablocki, Éloi and Pérez, Patrick and Cord, Matthieu and Alahari, Karteek}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{LiDARTouch}: Monocular metric depth estimation with a few-beam LiDAR}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVIU}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lara-480.webp 480w,/assets/img/publication_preview/lara-800.webp 800w,/assets/img/publication_preview/lara-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/lara.png" class="preview z-depth-1" width="100%" height="auto" alt="lara.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bartoccioni2022lara" class="col-sm-8"> <div class="title">LaRa: Latents and Rays for Multi-Camera Bird’s-Eye-View Semantic Segmentation</div> <div class="author"> <a href="https://f-barto.github.io/" rel="external nofollow noopener" target="_blank">Florent Bartoccioni</a>, <em>Éloi Zablocki</em>, <a href="https://abursuc.github.io/" rel="external nofollow noopener" target="_blank">Andrei Bursuc</a>, <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a>, <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a>, and <a href="https://lear.inrialpes.fr/people/alahari/" rel="external nofollow noopener" target="_blank">Karteek Alahari</a> </div> <div class="periodical"> <em>In CoRL</em>, 2022 </div> <div class="periodical"> </div> <div class="abstract"> <em> The Perceiver architecture, combined with careful ray encoding, excels in multi-camera fusion and transforming perceptive views into bird’s-eye-view semantic segmentation. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2206.13294" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/lara" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bartoccioni2022lara</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bartoccioni, Florent and Zablocki, Éloi and Bursuc, Andrei and Pérez, Patrick and Cord, Matthieu and Alahari, Karteek}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{LaRa}: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CoRL}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/steex-480.webp 480w,/assets/img/publication_preview/steex-800.webp 800w,/assets/img/publication_preview/steex-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/steex.png" class="preview z-depth-1" width="100%" height="auto" alt="steex.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jacob2022steex" class="col-sm-8"> <div class="title">STEEX: Steering Counterfactual Explanations with Semantics</div> <div class="author"> <a href="https://scholar.google.fr/citations?user=BDXMtPy4fmYC" rel="external nofollow noopener" target="_blank">Paul Jacob</a>, <em>Éloi Zablocki</em>, <a href="https://scholar.google.fr/citations?user=IFLcfvUAAAAJ" rel="external nofollow noopener" target="_blank">Hédi Ben-Younes</a>, <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" rel="external nofollow noopener" target="_blank">Mickaël Chen</a>, <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>In ECCV</em>, 2022 </div> <div class="periodical"> </div> <div class="abstract"> <em> Using a well-structured image generative model unlocks the generation of counterfactual explanations for deep vision models dealing with high-quality image and complex scenes. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2111.09094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtu.be/79SMlEtscuY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/valeoai/steex" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jacob2022steex</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jacob, Paul and Zablocki, Éloi and Ben-Younes, Hédi and Chen, Mickaël and Pérez, Patrick and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{STEEX}: Steering Counterfactual Explanations with Semantics}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cab-480.webp 480w,/assets/img/publication_preview/cab-800.webp 800w,/assets/img/publication_preview/cab-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/cab.png" class="preview z-depth-1" width="100%" height="auto" alt="cab.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="benyounes2022cab" class="col-sm-8"> <div class="title">Raising context awareness in motion forecasting</div> <div class="author"> <a href="https://scholar.google.fr/citations?user=IFLcfvUAAAAJ" rel="external nofollow noopener" target="_blank">Hedi Ben-Younes<sup>*</sup></a>, <em>Éloi Zablocki<sup>*</sup></em>, <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" rel="external nofollow noopener" target="_blank">Mickaël Chen</a>, <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a>, and <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a> </div> <div class="periodical"> <em>In CVPR Workshop on Autonomous Driving (WAD)</em>, 2022 </div> <div class="periodical"> </div> <div class="abstract"> <em> As trajectory prediction models merely extrapolate past motion, CAB enhances the use of HD-map information, addressing long-tail corner cases. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2109.08048" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/cab" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">benyounes2022cab</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ben-Younes, Hedi and Zablocki, Éloi and Chen, Mickaël and Cord, Matthieu and Pérez, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Raising context awareness in motion forecasting}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CVPR Workshop on Autonomous Driving (WAD)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/xai_survey-480.webp 480w,/assets/img/publication_preview/xai_survey-800.webp 800w,/assets/img/publication_preview/xai_survey-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/xai_survey.png" class="preview z-depth-1" width="100%" height="auto" alt="xai_survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zablocki2022xai_driving_survey" class="col-sm-8"> <div class="title">Explainability of deep vision-based autonomous driving systems: Review and challenges</div> <div class="author"> <em>Éloi Zablocki<sup>*</sup></em>, <a href="https://scholar.google.fr/citations?user=IFLcfvUAAAAJ" rel="external nofollow noopener" target="_blank">Hédi Ben-Younes<sup>*</sup></a>, <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a>, and <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a> </div> <div class="periodical"> <em>In IJCV</em>, 2022 </div> <div class="periodical"> </div> <div class="abstract"> <em> A survey on explainability methods for vision-based autonomous-driving models. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2101.05307" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zablocki2022xai_driving_survey</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zablocki, Éloi and Ben-Younes, Hédi and Pérez, Patrick and Cord, Matthieu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explainability of deep vision-based autonomous driving systems: Review and challenges}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IJCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/beef-480.webp 480w,/assets/img/publication_preview/beef-800.webp 800w,/assets/img/publication_preview/beef-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/beef.png" class="preview z-depth-1" width="100%" height="auto" alt="beef.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="benyounes2022beef" class="col-sm-8"> <div class="title">Driving Behavior Explanation with Multi-level Fusion</div> <div class="author"> <a href="https://scholar.google.fr/citations?user=IFLcfvUAAAAJ" rel="external nofollow noopener" target="_blank">Hedi Ben-Younes<sup>*</sup></a>, <em>Éloi Zablocki<sup>*</sup></em>, <a href="https://cord.isir.upmc.fr/" rel="external nofollow noopener" target="_blank">Matthieu Cord</a>, and <a href="https://ptrckprz.github.io/" rel="external nofollow noopener" target="_blank">Patrick Pérez</a> </div> <div class="periodical"> <em>In Pattern Recognition journal (PR)</em>, 2022 </div> <div class="periodical"> </div> <div class="abstract"> <em> BEEF is a self-driving model that both drives and explains its decisions with natural language. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2012.04983" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/valeoai/beef" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">benyounes2022beef</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ben-Younes, Hedi and Zablocki, Éloi and Cord, Matthieu and Pérez, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Driving Behavior Explanation with Multi-level Fusion}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Pattern Recognition journal (PR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/transductive_zsl.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/transductive_zsl.PNG" class="preview z-depth-1" width="100%" height="auto" alt="transductive_zsl.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bordes2020transductive" class="col-sm-8"> <div class="title">Transductive Zero-Shot Learning using Cross-modal CycleGAN</div> <div class="author"> <a href="https://www.linkedin.com/in/patrick-bordes-052b1a12b/" rel="external nofollow noopener" target="_blank">Patrick Bordes</a>, <em>Éloi Zablocki</em>, <a href="https://www.piwowarski.fr/" rel="external nofollow noopener" target="_blank">Benjamin Piwowarski</a>, and <a href="https://pages.isir.upmc.fr/gallinari/" rel="external nofollow noopener" target="_blank">Patrick Gallinari</a> </div> <div class="periodical"> <em>arxiv</em>, 2020 </div> <div class="periodical"> </div> <div class="abstract"> <em> Using a cycle-consistency loss reduces the domain shift between visual and textual representations, enhancing performance in zero-shot object recognition. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/2011.06850" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bordes2020transductive</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bordes, Patrick and Zablocki, Éloi and Piwowarski, Benjamin and Gallinari, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transductive Zero-Shot Learning using Cross-modal {CycleGAN}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arxiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/czsl.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/czsl.PNG" class="preview z-depth-1" width="100%" height="auto" alt="czsl.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zablocki2019context" class="col-sm-8"> <div class="title">Context-Aware Zero-Shot Learning for Object Recognition</div> <div class="author"> <em>Éloi Zablocki<sup>*</sup></em>, <a href="https://www.linkedin.com/in/patrick-bordes-052b1a12b/" rel="external nofollow noopener" target="_blank">Patrick Bordes<sup>*</sup></a>, <a href="https://www.piwowarski.fr/" rel="external nofollow noopener" target="_blank">Benjamin Piwowarski</a>, <a href="https://pages.isir.upmc.fr/soulier/" rel="external nofollow noopener" target="_blank">Laure Soulier</a>, and <a href="https://pages.isir.upmc.fr/gallinari/" rel="external nofollow noopener" target="_blank">Patrick Gallinari</a> </div> <div class="periodical"> <em>In ICML</em>, 2019 </div> <div class="periodical"> </div> <div class="abstract"> <em> Using visual context boosts zero-shot object recognition. </em> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Extended Oral</a> <a href="http://arxiv.org/abs/1904.12638" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Extended Oral</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zablocki2019context</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zablocki, Éloi and Bordes, Patrick and Piwowarski, Benjamin and Soulier, Laure and Gallinari, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Context-Aware Zero-Shot Learning for Object Recognition}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICML}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/grounded_sentence_embedding.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/grounded_sentence_embedding.PNG" class="preview z-depth-1" width="100%" height="auto" alt="grounded_sentence_embedding.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bordes2019incorporating" class="col-sm-8"> <div class="title">Incorporating Visual Semantics into Sentence Representations within a Grounded Space</div> <div class="author"> <a href="https://www.linkedin.com/in/patrick-bordes-052b1a12b/" rel="external nofollow noopener" target="_blank">Patrick Bordes<sup>*</sup></a>, <em>Éloi Zablocki<sup>*</sup></em>, <a href="https://pages.isir.upmc.fr/soulier/" rel="external nofollow noopener" target="_blank">Laure Soulier</a>, <a href="https://www.piwowarski.fr/" rel="external nofollow noopener" target="_blank">Benjamin Piwowarski</a>, and <a href="https://pages.isir.upmc.fr/gallinari/" rel="external nofollow noopener" target="_blank">Patrick Gallinari</a> </div> <div class="periodical"> <em>In EMNLP</em>, 2019 </div> <div class="periodical"> </div> <div class="abstract"> <em> A careful transfer of visual features to sentence representations enriches the semantics of general-purpose textual representations. </em> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Oral</a> <a href="http://arxiv.org/abs/2002.02734" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Oral</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bordes2019incorporating</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bordes, Patrick and Zablocki, Éloi and Soulier, Laure and Piwowarski, Benjamin and Gallinari, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Incorporating Visual Semantics into Sentence Representations within a Grounded Space}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EMNLP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/grounded_word_embedding.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/grounded_word_embedding.PNG" class="preview z-depth-1" width="100%" height="auto" alt="grounded_word_embedding.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zablocki2018learning" class="col-sm-8"> <div class="title">Learning Multi-Modal Word Representation Grounded in Visual Context</div> <div class="author"> <em>Éloi Zablocki</em>, <a href="https://www.piwowarski.fr/" rel="external nofollow noopener" target="_blank">Benjamin Piwowarski</a>, <a href="https://pages.isir.upmc.fr/soulier/" rel="external nofollow noopener" target="_blank">Laure Soulier</a>, and <a href="https://pages.isir.upmc.fr/gallinari/" rel="external nofollow noopener" target="_blank">Patrick Gallinari</a> </div> <div class="periodical"> <em>In AAAI</em>, 2018 </div> <div class="periodical"> </div> <div class="abstract"> <em> Visual context can be used, along with textual context, to learn improved word representations with the skip-gram algorithm. </em> </div> <div class="links"> <a href="http://arxiv.org/abs/1711.03483" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zablocki2018learning</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zablocki, Éloi and Piwowarski, Benjamin and Soulier, Laure and Gallinari, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Multi-Modal Word Representation Grounded in Visual Context}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AAAI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/clef.PNG" sizes="200px"></source> <img src="/assets/img/publication_preview/clef.PNG" class="preview z-depth-1" width="100%" height="auto" alt="clef.PNG" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zablocki2017sprl" class="col-sm-8"> <div class="title">LIP6@CLEF2017: Multi-Modal Spatial Role Labeling using Word Embeddings</div> <div class="author"> <em>Éloi Zablocki</em>, <a href="https://www.linkedin.com/in/patrick-bordes-052b1a12b/" rel="external nofollow noopener" target="_blank">Patrick Bordes</a>, <a href="https://pages.isir.upmc.fr/soulier/" rel="external nofollow noopener" target="_blank">Laure Soulier</a>, <a href="https://www.piwowarski.fr/" rel="external nofollow noopener" target="_blank">Benjamin Piwowarski</a>, and <a href="https://pages.isir.upmc.fr/gallinari/" rel="external nofollow noopener" target="_blank">Patrick Gallinari</a> </div> <div class="periodical"> <em>In CLEF</em>, 2017 </div> <div class="periodical"> </div> <div class="abstract"> <em> A linear SVM on pooled word representations to classify spatial relations from text and images. </em> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ceur-ws.org/Vol-1866/paper_76.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zablocki2017sprl</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zablocki, Éloi and Bordes, Patrick and Soulier, Laure and Piwowarski, Benjamin and Gallinari, Patrick}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{LIP6@CLEF2017}: Multi-Modal Spatial Role Labeling using Word Embeddings}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CLEF}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Éloi Zablocki. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>